{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gpu on the gpu machine\n",
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=1\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex\n",
    "# install the libraries necessary for data wrangling, prediction and result analysis\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score,precision_score, recall_score\n",
    "import torch\n",
    "from numba import cuda\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "import wandb\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Login to wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-GENRE-2\n",
    "train_df = pd.read_csv(\"/kaggle/input/xgenre2/X-GENRE-train.csv-2.csv\", index_col=0)\n",
    "dev_df = pd.read_csv(\"/kaggle/input/xgenre2/X-GENRE-dev.csv-2.csv\",  index_col = 0)\n",
    "test_df = pd.read_csv(\"/kaggle/input/xgenre2/X-GENRE-test.csv-2.csv\", index_col = 0)\n",
    "\n",
    "print(\"X-GENRE-2 train shape: {}, Dev shape: {}, Test shape: {}.\".format(train_df.shape, dev_df.shape, test_df.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of labels\n",
    "LABELS = train_df.labels.unique().tolist()\n",
    "print(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Wandb\n",
    "wandb.init(project=\"X-GENRE classifiers\", entity=\"tajak\", name=\"X-GENRE-2-training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate how many steps will each epoch have\n",
    "# Num steps in epoch = training samples / batch size\n",
    "steps_per_epoch = int(1562/8)\n",
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TransformerModel\n",
    "roberta_base_model = ClassificationModel(\n",
    "        \"xlmroberta\", \"xlm-roberta-base\",\n",
    "        num_labels=len(LABELS),\n",
    "        use_cuda=True,\n",
    "        args= {\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"num_train_epochs\": 30,\n",
    "            \"train_batch_size\":8,\n",
    "            \"learning_rate\": 1e-5,\n",
    "            # Use these parameters if you want to evaluate during training\n",
    "            \"evaluate_during_training\": True,\n",
    "            \"evaluate_during_training_steps\": steps_per_epoch*10,\n",
    "            \"evaluate_during_training_verbose\": True,\n",
    "            \"use_cached_eval_features\": True,\n",
    "            'reprocess_input_data': True,\n",
    "            \"labels_list\": LABELS,\n",
    "            # The following parameters are commented out because I want to save the model\n",
    "            \"no_cache\": True,\n",
    "            # Disable no_save: True if you want to save the model\n",
    "            \"no_save\": True,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"save_steps\": -1,\n",
    "            # Only the trained model will be saved - to prevent filling all of the space\n",
    "            \"save_model_every_epoch\":False,\n",
    "            \"wandb_project\": 'X-GENRE classifiers',\n",
    "            \"silent\": True,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "roberta_base_model.train_model(train_df, eval_df = dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(test_df, test_name, epoch):\n",
    "    \"\"\"\n",
    "    This function takes the test dataset and applies the trained model on it to infer predictions.\n",
    "    It also prints and saves a confusion matrix, calculates the F1 scores and saves the results in a list of results.\n",
    "\n",
    "    Args:\n",
    "    - test_df (pandas DataFrame)\n",
    "    - test_name\n",
    "    - epoch: num_train_epochs\n",
    "    \"\"\"\n",
    "    # Get the true labels\n",
    "    y_true = test_df.labels\n",
    "\n",
    "    model = roberta_base_model\n",
    "    \n",
    "    # Calculate the model's predictions on test\n",
    "    def make_prediction(input_string):\n",
    "        return model.predict([input_string])[0][0]\n",
    "\n",
    "    y_pred = test_df.text.apply(make_prediction)\n",
    "\n",
    "    # Calculate the scores\n",
    "    macro = f1_score(y_true, y_pred, labels=LABELS, average=\"macro\")\n",
    "    micro = f1_score(y_true, y_pred, labels=LABELS,  average=\"micro\")\n",
    "    print(f\"Macro f1: {macro:0.3}, Micro f1: {micro:0.3}\")\n",
    "\n",
    "    # Plot the confusion matrix:\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=LABELS)\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    plt.imshow(cm, cmap=\"Oranges\")\n",
    "    for (i, j), z in np.ndenumerate(cm):\n",
    "        plt.text(j, i, '{:d}'.format(z), ha='center', va='center')\n",
    "    classNames = LABELS\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    tick_marks = np.arange(len(classNames))\n",
    "    plt.xticks(tick_marks, classNames, rotation=90)\n",
    "    plt.yticks(tick_marks, classNames)\n",
    "    plt.title(f\"{test_name}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig1 = plt.gcf()\n",
    "    plt.show()\n",
    "    plt.draw()\n",
    "    #fig1.savefig(f\"Confusion-matrix-{test_name}.png\",dpi=100)\n",
    "\n",
    "    # Save the results:\n",
    "    rezdict = {\n",
    "        \"experiment\": test_name,\n",
    "        \"num_train_epochs\": epoch,\n",
    "        \"train_batch_size\":8,\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"microF1\": micro,\n",
    "        \"macroF1\": macro,\n",
    "        \"y_true\": y_true.to_dict(),\n",
    "        \"y_pred\": y_pred.to_dict(),\n",
    "        }\n",
    "    #previous_results.append(rezdict)\n",
    "\n",
    "    #Save intermediate results (just in case)\n",
    "    backup = []\n",
    "    backup.append(rezdict)\n",
    "    with open(f\"backup-results-{test_name}.json\", \"w\") as backup_file:\n",
    "        json.dump(backup,backup_file, indent= \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for various epochs to find the optimum number\n",
    "epochs = [2, 5, 8, 10, 15, 20]\n",
    "\n",
    "for epoch in epochs:\n",
    "    roberta_base_model = ClassificationModel(\n",
    "                \"xlmroberta\", \"xlm-roberta-base\",\n",
    "                num_labels=len(LABELS),\n",
    "                use_cuda=True,\n",
    "                args= {\n",
    "                    \"overwrite_output_dir\": True,\n",
    "                    \"num_train_epochs\": epoch,\n",
    "                    \"train_batch_size\":8,\n",
    "                    \"learning_rate\": 1e-5,\n",
    "                    \"labels_list\": LABELS,\n",
    "                    # The following parameters (no_cache, no_save) are commented out if I want to save the model\n",
    "                    \"no_cache\": True,\n",
    "                    # Disable no_save: True if you want to save the model\n",
    "                    \"no_save\": True,\n",
    "                    \"max_seq_length\": 512,\n",
    "                    \"save_steps\": -1,\n",
    "                    # Only the trained model will be saved - to prevent filling all of the space\n",
    "                    \"save_model_every_epoch\":False,\n",
    "                    \"wandb_project\": 'X-GENRE classifiers',\n",
    "                    \"silent\": True,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Train the model\n",
    "    roberta_base_model.train_model(train_df)\n",
    "    \n",
    "    # Test the model on dev_df\n",
    "    testing(dev_df, f\"X-GENRE-dev-epoch-search:{epoch}\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the results by creating a dataframe from the previous_results dictionary:\n",
    "results_df = pd.DataFrame(previous_results)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TransformerModel\n",
    "roberta_base_model = ClassificationModel(\n",
    "        \"xlmroberta\", \"xlm-roberta-base\",\n",
    "        num_labels=len(LABELS),\n",
    "        use_cuda=True,\n",
    "        args= {\n",
    "            \"overwrite_output_dir\": True,\n",
    "            \"num_train_epochs\": 8,\n",
    "            \"train_batch_size\":8,\n",
    "            \"learning_rate\": 1e-5,\n",
    "            \"labels_list\": LABELS,\n",
    "            # The following parameters are commented out because I want to save the model\n",
    "            #\"no_cache\": True,\n",
    "            # Disable no_save: True if you want to save the model\n",
    "            #\"no_save\": True,\n",
    "            \"max_seq_length\": 512,\n",
    "            \"save_steps\": -1,\n",
    "            # Only the trained model will be saved - to prevent filling all of the space\n",
    "            \"save_model_every_epoch\":False,\n",
    "            \"wandb_project\": 'X-GENRE classifiers',\n",
    "            \"silent\": True,\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "roberta_base_model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "# Import the final dataset with test sets\n",
    "with open(\"manual-annotations/multilingual-genre-annotated-test-set.json\") as main_file:\n",
    "\tmain_dict = json.load(main_file)\n",
    "\n",
    "main_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "from scipy.special import softmax\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "import argparse\n",
    "from knockknock import discord_sender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_genre(texts):\n",
    "    prediction_list = []\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"classla/xlm-roberta-base-multilingual-text-genre-classifier\")\n",
    "    model.to(\"cuda:0\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"classla/xlm-roberta-base-multilingual-text-genre-classifier\")\n",
    "\n",
    "    labels = [\"Other\", \"Information/Explanation\", \"News\", \"Instruction\", \"Opinion/Argumentation\", \"Forum\", \"Prose/Lyrical\", \"Legal\", \"Promotion\"]\n",
    "\n",
    "    def transcode(logit):\n",
    "        cats=sorted(zip(labels,softmax(logit)),key=lambda x:-x[1])\n",
    "        if cats[0][1]>=0.8:\n",
    "            label=cats[0][0]\n",
    "        else:\n",
    "            label='Mix'\n",
    "        return label\n",
    "\n",
    "    inputs = tokenizer(texts, max_length=512, truncation=True, padding=True, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "    for idx in range(len(logits)):\n",
    "        current_logit = logits[idx].tolist()\n",
    "        prediction_list.append(transcode(current_logit))\n",
    "    \n",
    "    print(\"Prediction finished.\")\n",
    "\n",
    "    return prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['translation','shuffled-text', 'text_no_punct', 'text_no_capital','text_no_capital_rand', 'text_no_num', 'text_no_num_rand','text_no_structure']:\n",
    "  text_list = df[column].to_list()\n",
    "  print(f\"Predicting genres to column {column}\")\n",
    "  prediction_list = predict_genre(text_list)\n",
    "  df[f\"{column}-pred\"] = prediction_list\n",
    "\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from evaluation import testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the test set\n",
    "df = pd.read_csv(\"datasets/adversarial-analysis/merged-test-sets-for-adversarial-analysis.csv\", index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['y_true', 'translation-pred', 'shuffled-text-pred','text_no_punct-pred', 'text_no_capital-pred','text_no_capital_rand-pred', 'text_no_num-pred','text_no_num_rand-pred', 'text_no_structure-pred', 'text_random_removal_10-pred',\n",
    "       'text_random_removal_25-pred', 'text_random_removal_50-pred']:\n",
    "\tprint(f\"{column}\\n\\n\")\n",
    "\tmerge_stats = pd.concat((df[column].value_counts(normalize=True), df[column].value_counts()), axis=1)\n",
    "\tmerge_stats.rename(columns = {\"count\": f\"count_{column}\"}, inplace=True)\n",
    "\n",
    "\tif column not in ['text_no_capital-pred', 'text_no_num-pred','text_no_num_rand-pred', 'text_no_structure-pred']:\n",
    "\t\tmerge_stats = pd.concat((merge_stats, df[\"y_pred\"].value_counts()), axis=1)\n",
    "\t\tmerge_stats.rename(columns = {\"count\": \"count_y_pred\"}, inplace=True)\n",
    "\t\tif column != \"y_true\":\n",
    "\t\t\tmerge_stats = pd.concat((merge_stats, df[\"y_true\"].value_counts()), axis=1)\n",
    "\t\t\tmerge_stats.rename(columns = {\"count\": \"count_y_true\"}, inplace=True)\n",
    "\t\t\tmerge_stats[\"change vs y_true (%)\"] = ((merge_stats[f\"count_{column}\"] - merge_stats[\"count_y_true\"])/merge_stats[f\"count_y_true\"]*100)\n",
    "\t\tmerge_stats[\"change vs y_pred (%)\"] = ((merge_stats[f\"count_{column}\"] - merge_stats[\"count_y_pred\"])/merge_stats[f\"count_y_pred\"]*100)\n",
    "\t\tprint(merge_stats.sort_values(by=\"change vs y_pred (%)\", ascending=False).to_markdown())\n",
    "\t\tprint(\"\\n --------------------------- \\n\")\n",
    "\telif column == 'text_no_capital-pred':\n",
    "\t\tmerge_stats = pd.concat((merge_stats, df[\"text_no_capital_rand-pred\"].value_counts()), axis=1)\n",
    "\t\tmerge_stats.rename(columns = {\"count\": \"count_no_capital_rand\"}, inplace=True)\n",
    "\t\tmerge_stats = pd.concat((merge_stats, df[\"y_true\"].value_counts()), axis=1)\n",
    "\t\tmerge_stats.rename(columns = {\"count\": \"count_y_true\"}, inplace=True)\n",
    "\t\tmerge_stats[\"change vs y_true (%)\"] = ((merge_stats[f\"count_{column}\"] - merge_stats[\"count_y_true\"])/merge_stats[f\"count_y_true\"]*100)\n",
    "\t\tmerge_stats[\"change vs no_capital_rand (%)\"] = ((merge_stats[f\"count_{column}\"] - merge_stats[\"count_no_capital_rand\"])/merge_stats[f\"count_no_capital_rand\"]*100)\n",
    "\t\tprint(merge_stats.sort_values(by=\"change vs no_capital_rand (%)\", ascending=False).to_markdown())\n",
    "\t\tprint(\"\\n --------------------------- \\n\")\n",
    "\telif column in ['text_no_num-pred', 'text_no_structure-pred']:\n",
    "\t\tmerge_stats = pd.concat((merge_stats, df['text_no_num_rand-pred'].value_counts()), axis=1)\n",
    "\t\tmerge_stats.rename(columns = {\"count\": \"count_no_num_rand\"}, inplace=True)\n",
    "\t\tmerge_stats = pd.concat((merge_stats, df[\"y_true\"].value_counts()), axis=1)\n",
    "\t\tmerge_stats.rename(columns = {\"count\": \"count_y_true\"}, inplace=True)\n",
    "\t\tmerge_stats[\"change vs y_true (%)\"] = ((merge_stats[f\"count_{column}\"] - merge_stats[\"count_y_true\"])/merge_stats[f\"count_y_true\"]*100)\n",
    "\t\tmerge_stats[\"change vs no_num_rand (%)\"] = ((merge_stats[f\"count_{column}\"] - merge_stats[\"count_no_num_rand\"])/merge_stats[f\"count_no_num_rand\"]*100)\n",
    "\t\tprint(merge_stats.sort_values(by=\"change vs no_num_rand (%)\", ascending=False).to_markdown())\n",
    "\t\tprint(\"\\n --------------------------- \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for column in [\"y_pred\", 'translation-pred', 'shuffled-text-pred', 'text_no_punct-pred', 'text_no_capital-pred', 'text_no_capital_rand-pred', 'text_no_num-pred', 'text_no_num_rand-pred', 'text_no_structure-pred', 'text_random_removal_10-pred', 'text_random_removal_25-pred', 'text_random_removal_50-pred']:\n",
    "\tprint(column)\n",
    "\tprint(\"\\n\\n\")\n",
    "\ty_pred = df[column].to_list()\n",
    "\ty_true = df[\"y_true\"].to_list()\n",
    "\tlabels = list(df[\"y_true\"].unique())\n",
    "\n",
    "\tcurrent_result = testing(y_true, y_pred, labels, show_matrix=True)\n",
    "\n",
    "\tprint(current_result)\n",
    "\n",
    "\tcurrent_dict = {}\n",
    "\tcurrent_dict[\"macro_F1\"] = current_result[\"macro F1\"]\n",
    "\n",
    "\tfor label in labels:\n",
    "\t\ttry:\n",
    "\t\t\tcurrent_dict[f\"{label}_F1\"] = current_result[\"report\"][label][\"f1-score\"]\n",
    "\t\texcept:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\tresults[column] = current_dict\n",
    "\tprint(\"---------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
